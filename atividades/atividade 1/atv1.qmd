---
title: Regressão linear, polinomial e regularização
jupyter: python3
---

# Questao 1

## a) Apresente os parâmetros do modelo e o MSE (erro quadrático médio) obtidos pelo algoritmo OLS (mínimos quadrados ordinários). Plote a reta resultante sobre os dados.


```{python}
import numpy as np
import matplotlib.pyplot as plt

# Carregando o arquivo
atificial_data = np.loadtxt("artificial1d.csv", delimiter=",")
x = atificial_data[:, 0]
y = atificial_data[:, 1]


print(f"x:{x} \n")
print(f"y:{y} \n")
```


```{python}
plt.scatter(x, y, label="Dados")
plt.xlabel('x')
plt.ylabel('y')
plt.title('Dados artificiais 1D')
plt.legend()
plt.grid(True)
plt.show()
```


```{python}
X = np.c_[np.ones(x.shape[0]), x]

w = (np.linalg.inv(X.T @ X) @ X.T @ y)

w_0, w_1 = w
print(f"Parâmetros encontrados: w0 = {w_0:.4f}, w1 = {w_1:.4f}")

```


```{python}
def MSE(y_real, y_est):
    return np.mean((y_real - y_est)**2)

y_pred = X @ w

print(MSE(y, y_pred))
```
 
```{python}
plt.scatter(x, y, label="Dados")
plt.plot(x, y_pred, color='red', label="Reta OLS")
plt.xlabel('x')
plt.ylabel('y')
plt.title('Regressão Linear (OLS)')
plt.legend()
plt.grid(True)
plt.show()
```


## b) Apresente os parâmetros do modelo, o MSE e a curva de aprendizagem obtidos pelo algoritmo GD (gradiente descendente). Plote a reta resultante sobre os dados.


```{python}
def gd(X, y, alpha=0.1, epochs=100):
    m, n = X.shape
    w = np.zeros(n)
    mse_history = []

    for _ in range(epochs):
        y_pred = X @ w
        erro = y_pred - y
        gradiente = (2/m) * X.T @ erro
        w -= alpha * gradiente
        mse_history.append(np.mean(erro**2))
        
    return w, mse_history

# Executa o gradiente descendente
w_gd, mse_gd = gd(X, y, alpha=0.1, epochs=100)

w0_gd, w1_gd = w_gd
print(f"Parâmetros GD: w0 = {w0_gd:.4f}, w1 = {w1_gd:.4f}")
```


```{python}
y_pred_gd = X @ w_gd

print("MSE GD:", MSE(y, y_pred_gd))
```


```{python}

plt.scatter(x, y, label="Dados")
plt.plot(x, y_pred_gd, color='green', label="Reta GD")
plt.xlabel('x')
plt.ylabel('y')
plt.title('Regressão Linear com Gradiente Descendente')
plt.legend()
plt.grid(True)
plt.show()
```


```{python}
plt.plot(mse_gd)
plt.xlabel("Época")
plt.ylabel("MSE")
plt.title("Evolução do erro (MSE) - GD")
plt.grid(True)
plt.show()
```

## c) Apresente os parâmetros do modelo, o MSE e a curva de aprendizagem obtidos pelo algoritmo SGD (gradiente descendente estocástico). Plote a reta resultante sobre os dados.


```{python}
def sgd(X, y, alpha=0.1, epochs=30):
    m, n = X.shape
    w = np.zeros(n)
    mse_history = []

    for epoch in range(epochs):
        indices = np.random.permutation(m)

        for i in indices:
            xi = X[i, :].reshape(1, -1)
            yi = y[i]
            yi_pred = xi @ w
            erro = yi_pred - yi
            gradiente = 2 * xi.T * erro
            w -= alpha * gradiente.flatten()

            # Calcula o MSE total em todo o dataset após essa atualização
            y_pred_total = X @ w
            mse = np.mean((y - y_pred_total) ** 2)
            mse_history.append(mse)

    return w, mse_history
```

```{python}
w_sgd, mse_sgd = sgd(X, y, alpha=0.01, epochs=50)

w0_sgd, w1_sgd = w_sgd
print(f"Parâmetros SGD: w0 = {w0_sgd:.4f}, w1 = {w1_sgd:.4f}")
```


```{python}
y_pred_sgd = X @ w_sgd
print("MSE SGD:", MSE(y, y_pred_sgd))
```

```{python}
plt.scatter(x, y, label="Dados")
plt.plot(x, y_pred_sgd, color='purple', label="Reta SGD")
plt.xlabel('x')
plt.ylabel('y')
plt.title('Regressão Linear com Gradiente Estocástico')
plt.legend()
plt.grid(True)
plt.show()
```

```{python}
plt.plot(mse_sgd)
plt.xlabel("Época")
plt.ylabel("MSE")
plt.title("Evolução do erro (MSE) - SGD")
plt.grid(True)
plt.show()
```


