---
title: Introdução ao `numpy`
jupyter: python3
---



```{python}
#| colab: {}
#| colab_type: code
import numpy as np
import matplotlib.pyplot as plt  # Biblioteca para gerar gráficos
```

Vamos criar umas matrizes e vetores para começar...

$$\begin{aligned}
\boldsymbol{A} &= \begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}\ &
\boldsymbol{B} &= \begin{bmatrix}1 & 3\\ 5 & 7\end{bmatrix}\\\\
\boldsymbol{v}_1 &= \begin{bmatrix}5 & 3\end{bmatrix}\ &
\boldsymbol{v}_2 &= \begin{bmatrix}9 & 2 & 1\end{bmatrix}
\end{aligned}$$

```{python}
#| colab: {}
#| colab_type: code
A = np.array([[2,0],[4,6],[8,2]])
B = np.array([[1,3],[5,7]])
v1 = np.array([5,3])
v2 = np.array([9,2,1])
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 85}
#| colab_type: code
print("Dimensão de A:", A.shape)
print("Dimensão de B:", B.shape)
print("Dimensão de v1:", v1.shape)
print("Dimensão de v2:", v1.shape)
```

$$\begin{aligned}
\boldsymbol{A} &= \begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}\\
\boldsymbol{A}^{\top} &= \begin{bmatrix}2 & 4 & 8\\ 0 & 6 & 2\end{bmatrix}
\end{aligned}$$

```{python}
print("Matriz original:")
print(A)
print("Matriz transposta:")
print(A.T)
```

$$\begin{aligned}
\boldsymbol{A}[0] &= \begin{bmatrix}2 & 0\end{bmatrix}\\
\boldsymbol{A}[[0,2]] &= \begin{bmatrix}2 & 0\\ 8 & 2\end{bmatrix}\\
\end{aligned}$$

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 85}
#| colab_type: code
print("Primeira linha de A:")
print(A[0], "dimensão:", A[0].shape) # Primeira linha de A
print("Primeira e terceira linha de A:")
print(A[[0,2]], "dimensão:", A[[0,2]].shape) # Primeira e terceira linha de A
print("Primeira linha de A (mantendo 2 dimensões):")
print(A[[0]], "dimensão:", A[[0]].shape) # Primeira linha de A (mantendo 2 dimensões)
```

$$\begin{aligned}
\boldsymbol{A}[:,0] &= \begin{bmatrix}2 & 4 & 8\end{bmatrix}\\
\boldsymbol{A}[:,[0]] &= \begin{bmatrix}2 \\ 4 \\ 8\end{bmatrix}
\end{aligned}$$

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 136}
#| colab_type: code
print("Primeira coluna do A:")
print(A[:,0], "dimensão:", A[:,0].shape) # Primeira coluna do A
print("Primeira e segunda coluna do A:")
print(A[:,[0,1]], "dimensão:", A[:,[0,1]].shape) # Primeira e segunda coluna do A
print("Primeira coluna do A (mantendo 2 dimensões):")
print(A[:,[0]], "dimensão:", A[:,[0]].shape) # Primeira coluna do A (mantendo 2 dimensões)
```

$$\begin{aligned}
\left[\begin{bmatrix}2 & 4 & 8\end{bmatrix}; \begin{bmatrix}1 & 2 & 3\end{bmatrix}\right] &= \begin{bmatrix}2 & 4 & 8\\1 & 2 & 3\end{bmatrix}\\
\left[\begin{bmatrix}2 \\ 4 \\ 8\end{bmatrix}, \begin{bmatrix}1 \\ 2 \\ 3\end{bmatrix}\right] &= \begin{bmatrix}2 & 1 \\ 4 & 2 \\ 8 & 3\end{bmatrix}
\end{aligned}$$

```{python}
a = np.array([2, 4, 8]).reshape((1,-1))
b = np.array([1, 2, 3]).reshape((1,-1))
ab = np.r_[a,b]

print(a, "dimensão:", a.shape)
print(b, "dimensão:", b.shape)
print(ab, "dimensão:", ab.shape) # row stack
```

```{python}
c = np.array([2, 4, 8]).reshape((-1,1))
d = np.array([1, 2, 3]).reshape((-1,1))
cd = np.c_[c,d]

print(c, "dimensão:", c.shape)
print(d, "dimensão:", d.shape)
print(cd, "dimensão:", cd.shape) # column stack
```

## Matrizes notáveis

$$\begin{aligned}
\boldsymbol{I}_n = \mathtt{np.eye(n)} &= \begin{bmatrix}1 & 0 & \ldots\\ 0 & 1 & \ldots\\ \vdots&\vdots&\vdots\\ 0 & \ldots & 1\end{bmatrix}&
\boldsymbol{0}_{n,m} = \mathtt{np.zeros((n,m))} &= \begin{bmatrix}0 & 0 & \ldots\\ 0 & 0 & \ldots\\ \vdots&\vdots&\vdots\\ 0 & \ldots & 0\end{bmatrix}\\
\boldsymbol{1}_{n,m} = \mathtt{np.ones((n,m))} &= \begin{bmatrix}1 & 1 & \ldots\\ 1 & 1 & \ldots\\ \vdots&\vdots&\vdots\\ 1 & \ldots & 1\end{bmatrix}\\
\end{aligned}$$

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 238}
#| colab_type: code
print("Matriz identidade de ordem 5:")
print(np.eye(5))
print("Matriz de zeros 5x3:")
print(np.zeros((5,3)))
print("Matriz de uns 3x1:")
print(np.ones((3,1)))
```

Além dessas matrizes notáveis, também podemos fazer matrizes aleatórias:

`np.random.rand(n,m)` é uma matrix $n$ por $m$ onde $[\mathtt{np.random.rand(n,m)}]_{ij} \sim \mathcal{U}(0,1)$.

Ou seja, $[\mathtt{np.random.rand(n,m)}]_{ij} \in [0,1)$

`np.random.randn(n,m)` é uma matrix $n$ por $m$ onde $[\mathtt{np.random.randn(n,m)}]_{ij} \sim \mathcal{N}(0,1)$

Ou seja, $[\mathtt{np.random.randn(n,m)}]_{ij} \in (-\infty,+\infty)$

```{python}
print("Matriz 5x3 de números aleatórios amostrados de U(0,1):")
print(np.random.rand(5,3))
print("Matriz 2x2 de números aleatórios amostrados de N(0,1):")
print(np.random.randn(2,2))
```

## Operações lineares

Produto de escalares por matrizes:
$$\begin{aligned}
5\boldsymbol{A} &= \begin{bmatrix}2\cdot 5 & 0\cdot 5\\ 4\cdot 5 & 6\cdot 5\\ 8\cdot 5 & 2\cdot 5\end{bmatrix}\\[0.3em]
&= \begin{bmatrix}10 & 0\\ 20 & 30\\ 40 & 10\end{bmatrix}
\end{aligned}$$

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 68}
#| colab_type: code
print(5 * A)
```

Soma de matrizes:
$$\begin{aligned}
\boldsymbol{A}+\boldsymbol{A} &= \begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}+\begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}\\[0.3em]
&= \begin{bmatrix}4 & 0\\ 8 & 12\\ 26 & 4\end{bmatrix}
\end{aligned}$$

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 68}
#| colab_type: code
print(A + A)
```

Produto com broadcast (difusão) de um vetor por uma matriz:
$$\begin{aligned}
\boldsymbol{v}_1 \bullet \boldsymbol{A} &= \begin{bmatrix}5 & 3\end{bmatrix}\bullet\begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}\\[0.3em]
&= \begin{bmatrix}5\cdot 2 & 3\cdot 0\\ 5\cdot 4 & 3\cdot 6\\ 5\cdot 8 & 3\cdot 2\end{bmatrix}\\
&= \begin{bmatrix}10 & 0\\ 20 & 18\\ 40 & 6\end{bmatrix}\\
\end{aligned}$$

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 68}
#| colab_type: code
print(v1 * A)
```

Soma com broadcast (difusão) de um vetor por uma matriz:
$$\begin{aligned}
\boldsymbol{v}_1 \oplus \boldsymbol{A} &= \begin{bmatrix}5 & 3\end{bmatrix}\oplus\begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}\\[0.3em]
&= \begin{bmatrix}5 & 3\\ 5 & 3\\ 5 & 3\end{bmatrix}+\begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}\\
&= \begin{bmatrix}7 & 3\\ 9 & 9\\ 13 & 5\end{bmatrix}\\
\end{aligned}$$

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 68}
#| colab_type: code
print(v1 + A)
```

Cuidado com as dimensões das variáveis:

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 34}
#| colab_type: code
print("Vetor v2:")
print(v2, "dimensão:", v2.shape)
print("Matriz A:")
print(A, "dimensão:", A.shape)
try:
    v2 + A
except Exception as e:
    print(e)
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 34}
#| colab_type: code
print("Matriz A:")
print(A, "dimensão:", A.shape)
print("Matriz B:")
print(B, "dimensão:", B.shape)
try:
    A + B
except Exception as e:
    print(e)
```

## Operações não-lineares

$$\begin{aligned}
\boldsymbol{A} &= \begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}\\
\mathtt{A ** 2} &= \begin{bmatrix}2^2 & 0^2\\ 4^2 & 6^2\\ 8^2 & 2^2\end{bmatrix}
\end{aligned}$$

```{python}
print(A ** 2)
```

$$\begin{aligned}
\boldsymbol{A} &= \begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}\\
\mathtt{np.sqrt(A)} &= \begin{bmatrix}\sqrt{2} & \sqrt{0}\\ \sqrt{4} & \sqrt{6}\\ \sqrt{8} & \sqrt{2}\end{bmatrix}\\
\mathtt{A ** 0.5} &= \begin{bmatrix}2^\frac{1}{2} & 0^\frac{1}{2}\\ 4^\frac{1}{2} & 6^\frac{1}{2}\\ 8^\frac{1}{2} & 2^\frac{1}{2}\end{bmatrix}
\end{aligned}$$

```{python}
print(np.sqrt(A))
print(A ** 0.5)
```

## Operações de agregação

$$\begin{aligned}
\boldsymbol{A} &= \begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}\\
\mathtt{np.mean(A)} &= \frac{2 + 0 + 4 + 6 + 8 + 2}{2\cdot 3}\\
\mathtt{np.sum(A)} &= 2 + 0 + 4 + 6 + 8 + 2\\
\mathtt{np.prod(A)} &= 2 \cdot 0 \cdot 4 \cdot 6 \cdot 8 \cdot 2\\
\end{aligned}$$

```{python}
np.mean(A), np.sum(A), np.prod(A)
```

```{python}
A.mean(), A.sum(), A.prod()
```

$$\begin{aligned}
\boldsymbol{A} &= \begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}\\
\mathtt{np.mean(A, axis=0)} &= \frac{1}{3}\begin{bmatrix}2 + 4 + 8 & 0 + 6 + 2\end{bmatrix}\\
\mathtt{np.sum(A, axis=0)} &= \begin{bmatrix}2 + 4 + 8 & 0 + 6 + 2\end{bmatrix}\\
\mathtt{np.prod(A, axis=0)} &= \begin{bmatrix}2 \cdot 4 \cdot 8 & 0 \cdot 6 \cdot 2\end{bmatrix}\\
\end{aligned}$$

```{python}
np.mean(A, axis=0), np.sum(A, axis=0), np.prod(A, axis=0)
```

```{python}
A.mean(axis=0), A.sum(axis=0), A.prod(axis=0)
```

$$\begin{aligned}
\boldsymbol{A} &= \begin{bmatrix}2 & 0\\ 4 & 6\\ 8 & 2\end{bmatrix}\\
\mathtt{np.mean(A, axis=1)} &= \frac{1}{2}\begin{bmatrix}2 + 0 & 4 + 6 & 8 + 2\end{bmatrix}\\
\mathtt{np.sum(A, axis=1)} &= \begin{bmatrix}2 + 0 & 4 + 6 & 8 + 2\end{bmatrix}\\
\mathtt{np.prod(A, axis=1)} &= \begin{bmatrix}2 \cdot 0 & 4 \cdot 6 & 8 \cdot 2\end{bmatrix}\\
\end{aligned}$$

```{python}
np.mean(A, axis=1), np.sum(A, axis=1), np.prod(A, axis=1)
```

```{python}
A.mean(axis=1), A.sum(axis=1), A.prod(axis=1)
```

```{python}
np.mean(A, axis=0, keepdims=True)
```

```{python}
A.mean(axis=0, keepdims=True)
```

```{python}
A - A.mean(axis=0, keepdims=True)
```

## Multiplicação de Matrizes

Para matrizes $\boldsymbol{X} \in \mathbb{R}^{a\times b}$ e $\boldsymbol{Y} \in \mathbb{R}^{b\times c}$, temos que
$\boldsymbol{XY} \in \mathbb{R}^{a\times c}$. No Numpy, essa operação é representada por $\texttt{X @ Y}$.

A multiplicação é definida por:
$$\begin{aligned}
(\boldsymbol{XY})_{ij} = \sum_k \boldsymbol{X}_{ik}\boldsymbol{Y}_{kj}
\end{aligned}$$

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 85}
#| colab_type: code
print(A @ B)
print("Dimensão de A @ B:", (A@B).shape)
```

Cuidado com as dimensões das matrizes:

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 34}
#| colab_type: code
try:
    B @ A
except Exception as e:
    print(e)
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 51}
#| colab_type: code
print(A @ B @ v1)
print((A @ B) @ v1)
```

### Inversão de matrizes
Outra operação bastante comum é multiplicar uma matriz pela inversa dela...

Seja $\boldsymbol{B}$ uma matriz inversível, então $\boldsymbol{B}^{-1} \boldsymbol{B} = \boldsymbol{I}$

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 136}
#| colab_type: code
print(B)
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 136}
#| colab_type: code
B_inv = np.linalg.inv(B)
print(B_inv)
```

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 136}
#| colab_type: code
print(B_inv @ B)
```

A inversa pode ser usada no produto com vetores:
$\boldsymbol{B}^{-1} \boldsymbol{v}_1$

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 34}
#| colab_type: code
print(B_inv @ v1)
```

Porém, existe uma outra forma mais eficiente de computar a mesma coisa:

Se $\boldsymbol{B}^{-1} \boldsymbol{v}_1 = \boldsymbol{x}$, então $\boldsymbol{B} \boldsymbol{x} = \boldsymbol{v}_1$.
Ou seja, estamos resolvendo o sistema de equações lineares com coeficientes $\boldsymbol{B}$ e resultado $\boldsymbol{v}_1$

```{python}
#| colab: {base_uri: https://localhost:8080/, height: 34}
#| colab_type: code
print(np.linalg.solve(B,v1))
```

## Leitura de arquivos com dados

Normalmente os dados na disciplina serão recebidos no formato CSV. Isso significa que o dado é estruturado da seguinte maneira:
```
idade,pressão_sanguínea # Um cabeçalho opcional
39,144 # Dados separados por vírgulas ou outro separador
47,220
....
```

```{python}
#| colab: {}
#| colab_type: code
pressao_dataset = np.genfromtxt('./data/pressão.txt', delimiter=',', skip_header=1)
pressao_dataset
```

```
# idade, temperatura da água, comprimento
14,25,620
28,25,1315
....
```

```{python}
peixe_dataset = np.genfromtxt('./data/peixe.txt', delimiter=',')
peixe_dataset
```

## Exercícios

### Computar a fórmula da normalização escore-Z

Dado um conjunto de dados $\boldsymbol{X} = [\boldsymbol{x}_1, \boldsymbol{x}_1, \ldots, \boldsymbol{x}_N]$, a normalização por escore-Z é dada por:
$$\tilde{\boldsymbol{x_i}} = \frac{\boldsymbol{x_i} - \boldsymbol{\mu}}{\boldsymbol{\sigma}}$$

Onde:
$$\begin{aligned}
\boldsymbol{\mu} &= \frac{1}{N}\sum_i^N \boldsymbol{x_i}\\
\boldsymbol{\sigma} &= \sqrt{\frac{1}{N-1}\sum_i^N (\boldsymbol{x_i}-\boldsymbol{\mu})^2}\\
\end{aligned}$$

A "desnormalização" pode ser feita por:
$$\boldsymbol{x_i} = \boldsymbol{\sigma}\tilde{\boldsymbol{x_i}} + \boldsymbol{\mu}$$

Use a fórmula para normalizar o conjunto de dados `peixe` sem usar nenhum `for` ou `while`.

```{python}
#| tags: []
# Escreva o código aqui
x = peixe_dataset.copy()
x_mean = x.sum(axis=0) / x.shape[0]
x_std = np.sqrt(np.sum((x - x_mean)**2, axis=0) / (x.shape[0] - 1))
x_norm = (x - x_mean) / x_std

x_norm
```

### Encontrar raízes de funções

Dada uma função $f$, o seguinte procedimento (método de Newton-Raphson) consegue encontrar aproximar zeros desta função:

1. Inicialize $\tilde{x}_0$ com um chute inicial e escolha uma tolerância $\epsilon$;
2. Compute: $$\tilde{x}_{t} = \tilde{x}_{t-1} - \frac{f(\tilde{x}_{t-1})}{f'(\tilde{x}_{t-1})}$$
3. Repita o passo 2 até que $f(\tilde{x}_t) \leq \epsilon$.

Implemente esse procedimento para $f(x) = x^2 - 2$, teste seu resultado computando $\tilde{x} \cdot \tilde{x}$.

```{python}
#| tags: []
# Escreva o código aqui

def funcao (x):
  return x**2 - 2

def derivada_funcao (x):
  return 2*x

def newton (i):
  x_ant = i
  x_novo = x_ant - [funcao(x_ant) / derivada_funcao(x_ant)]
  return x_novo

def teste (x1,x2):
    return x1*x2 



```

# Regressão linear via mínimos quadrados ordinários (OLS)

Como visto na aula de regressão linear, dado o seguinte problema:
$$\begin{aligned}
\hat{\boldsymbol{y}} = \boldsymbol{X}\boldsymbol{w}\\
\text{Queremos encontrar:}\\
\hat{\boldsymbol{w}} = \arg\min_{\boldsymbol{w}} ||\boldsymbol{y} - \boldsymbol{X}\boldsymbol{w}||^2
\end{aligned}$$

Como visto, sabemos que:
$$\hat{\boldsymbol{w}} = (\boldsymbol{X}^{\top}\boldsymbol{X})^{-1} \boldsymbol{X}^{\top}\boldsymbol{y}$$

Obtenha um modelo linear para predizer o comprimento de um peixe a partir de sua idade e da temperatura da água. Para isso, encontre o $\hat{\boldsymbol{w}}$ pro dataset `peixe` onde:
$$\boldsymbol{X} = [\mathbf{1}^{\top} \;,\; \mathtt{peixe\underline{}dataset[:,[0,1]}]\\
\boldsymbol{y} = \mathtt{peixe\underline{}dataset[:,[2]]}$$

Sem utilizar estrutura de repetição alguma, calcule a raíz do erro quadrático médio:
$$\mathrm{RMSE} = \sqrt{\frac{1}{n}\sum_i^n (\boldsymbol{y} - \hat{\boldsymbol{y}})^2}$$

```{python}
#| tags: []
# Escreva o código aqui
```

# Regressão linear via gradiente descendente (GD)

Sendo $\alpha$ um passo de aprendizagem, $N$ o número de observações disponíveis e $t$ a iteração atual do algoritmo, os parâmetros $\boldsymbol{w}$ podem ser atualizados via algoritmo GD:

$$\boldsymbol{w}(t) = \boldsymbol{w}(t-1) + \alpha \frac{1}{N}\sum_{i=1}^{N} e_i(t-1) \boldsymbol{x}_i,$$
$$\text{em que } e_i(t-1) = y_i - \boldsymbol{w}^{\top}(t-1)\boldsymbol{x}_i$$

Normalize os dados retirando a média e dividindo pelo desvio padrão (entrada e saída). Na predição, desfaça a normalização para computar o RMSE.

```{python}
# Escreva o código aqui
```

# Regressão linear via gradiente descendente estocástico (SGD)

Sendo $\alpha$ um passo de aprendizagem, $N$ o número de observações disponíveis e $t$ a iteração atual do algoritmo, os parâmetros $\boldsymbol{w}$ podem ser atualizados via algoritmo SGD:

$$\boldsymbol{w}(t) = \boldsymbol{w}(t-1) + \alpha e_i(t-1) \boldsymbol{x}_i,$$
$$\text{em que } e_i(t-1) = y_i - \boldsymbol{w}^{\top}(t-1)\boldsymbol{x}_i$$

Normalize os dados retirando a média e dividindo pelo desvio padrão (entrada e saída). Na predição, desfaça a normalização para computar o RMSE.

```{python}
# Escreva o código aqui
```

